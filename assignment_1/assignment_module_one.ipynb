{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Members\n",
    "\n",
    "* **Student ID:** 0001111416  \n",
    "  **Full Name:** Alessio Pittiglio  \n",
    "  **Institutional Email:** alessio.pittiglio@studio.unibo.it\n",
    "\n",
    "* **Student ID:** 0001086355  \n",
    "  **Full Name:** Parsa Mastouri Kashani  \n",
    "  **Institutional Email:** parsa.mastouri@studio.unibo.it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Product Recognition of Books**\n",
    "\n",
    "## Image Processing and Computer Vision - Assignment Module \\#1\n",
    "\n",
    "\n",
    "Contacts:\n",
    "\n",
    "- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n",
    "- Prof. Samuele Salti -> samuele.salti@unibo.it\n",
    "- Alex Costanzino -> alex.costanzino@unibo.it\n",
    "- Francesco Ballerini -> francesco.ballerini4@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer vision-based object detection techniques can be applied in library or bookstore settings to build a system that identifies books on shelves.\n",
    "\n",
    "Such a system could assist in:\n",
    "* Helping visually impaired users locate books by title/author;\n",
    "* Automating inventory management (e.g., detecting misplaced or out-of-stock books);\n",
    "* Enabling faster book retrieval by recognizing spine text or cover designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Develop a computer vision system that, given a reference image for each book, is able to identify such book from one picture of a shelf.\n",
    "\n",
    "<figure>\n",
    "<a href=\"https://ibb.co/pvLVjbM5\"><img src=\"https://i.ibb.co/svVx9bNz/example.png\" alt=\"example\" border=\"0\"></a>\n",
    "</figure>\n",
    "\n",
    "For each type of product displayed on the shelf, the system should compute a bounding box aligned with the book spine or cover and report:\n",
    "1. Number of instances;\n",
    "1. Dimension of each instance (area in pixel of the bounding box that encloses each one of them);\n",
    "1. Position in the image reference system of each instance (four corners of the bounding box that enclose them);\n",
    "1. Overlay of the bounding boxes on the scene images.\n",
    "\n",
    "<font color=\"red\"><b>Each step of this assignment must be solved using traditional computer vision techniques.</b></font>\n",
    "\n",
    "#### Example of expected output\n",
    "```\n",
    "Book 0 - 2 instance(s) found:\n",
    "  Instance 1 {top_left: (100,200), top_right: (110, 220), bottom_left: (10, 202), bottom_right: (10, 208), area: 230px}\n",
    "  Instance 2 {top_left: (90,310), top_right: (95, 340), bottom_left: (24, 205), bottom_right: (23, 234), area: 205px}\n",
    "Book 1 – 1 instance(s) found:\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Two folders of images are provided:\n",
    "* **Models**: contains one reference image for each product that the system should be able to identify;\n",
    "* **Scenes**: contains different shelve pictures to test the developed algorithm in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !cp -r /content/drive/MyDrive/AssignmentsIPCV/dataset.zip ./\n",
    "# !unzip dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation criteria\n",
    "1. **Clarity and conciseness**. Present your work in a readable way: format your code and comment every important step;\n",
    "\n",
    "2. **Procedural correctness**. There are several ways to solve the assignment. Design your own sound approach and justify every decision you make;\n",
    "\n",
    "3. **Correctness of results**. Try to solve as many instances as possible. You should be able to solve all the instances of the assignment, however, a thoroughly justified and sound procedure with a lower number of solved instances will be valued **more** than a poorly designed and justified approach that solves more or all instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computer vision system we designed is based on a variant of the **Generalized Hough Transform (GHT)** that uses local invariant features such as SIFT. This approach is also often referred to as the Star Model. The system operates in two phases: an offline phase and an online phase.\n",
    "\n",
    "### Offline phase\n",
    "\n",
    "The goal of the offline phase is to build a model of the object to be detected starting from a template image.\n",
    "\n",
    "1. First, we extract the local invariant features from the template image.\n",
    "2. The object model is defined as a collection of these features. For each feature, we store its position $(x,y)$, its canonical orientation $\\theta$, its characteristic scale $s$, and its SIFT descriptor vector.\n",
    "3. A reference point is chosen for the object, typically the barycenter (centroid) of all keypoints.\n",
    "4. For each keypoint, a joining vector $r$ is computed. This vector points from the keypoint's position to the reference point and is stored as an additional attribute for each keypoint.\n",
    "\n",
    "At the end of this phase, we obtain the **Star Model**. The name comes from the fact that visually, these vectors all point toward the center, giving the appearance of a star.\n",
    "\n",
    "### Online phase\n",
    "\n",
    "The actual detection takes place during the online phase. The Star Model is used to find instances of the object in new images.\n",
    "\n",
    "1. The same type of local features are extracted from the target image.\n",
    "2. The descriptors of the target image's keypoints are matched against the descriptors of the keypoints stored in the Star Model. Lowe's ratio test is typically used to keep only the reliable matches and discard ambiguous ones.\n",
    "3. Voting represents the core of this method, where its strength truly lies. For each match, a vote is cast for the most probable position of the reference point in the target image. Each match also provides hypotheses for rotation and scale.\n",
    "    - For rotation, the difference between the canonical orientations of the target and model keypoints gives an estimate of the object's rotation.\n",
    "    - For scale, the ratio of the characteristic scales of the target and model keypoints gives an estimate of the object's scale change.\n",
    "    \n",
    "    Once these estimates are obtained, we transform the joining vector from the model keypoint and add it to the target keypoint's position to cast a vote for the reference point\n",
    "\n",
    "4. Votes are cast into a 2D accumulator. After all matches have voted, peaks in the accumulator array indicate the likely presence and position of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding box estimation phase\n",
    "\n",
    "To this already robust system, we added a third phase to estimate a bounding box for the object. Once the peak is found (using a local maximum detection algorithm on the grid), we introduce a data structure to keep track of which matches voted for that reference point. These matches are then used to estimate a homography.\n",
    "\n",
    "To make the system more robust, we also added a backup **similarity pose estimation** computed via LLS. Why? (1) It requires fewer points to estimate (3 instead of the 4 required for homography) and (2) it is quite reliable in our case since the objects we aim to detect are mostly rectangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional improvements\n",
    "\n",
    "Another improvement was the use of CLAHE for image pre-processing. This locally enhances the contrast of an image while limiting noise amplification through a “clip limit.” This resulted in an increased number of detected keypoints and, consequently, more votes in the accumulator.\n",
    "\n",
    "CLAHE can be applied in two different ways with completely different results\n",
    "- Apply it directly to the grayscale image.\n",
    "- Convert the image to the LAB color space, apply it only to the L channel, and then continue the pipeline using that L channel.\n",
    "\n",
    "In our case, empirical evaluation showed that the second approach produced better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse matching\n",
    "\n",
    "The standard SIFT algorithm, which uses Lowe's ratio test, is designed to find one-to-one correspondences between keypoints in the template and those in the target image. By its nature, it is poorly suited for scenarios where multiple instances need to be detected, especially when a model feature appears multiple times in the target image.\n",
    "\n",
    "For example, if a model keypoint has three matches in the scene, Lowe's ratio will keep at most one of them. In our early experiments, sequentially masking detected objects with a black box and re-running SIFT detection produced good results, but was extremely inefficient. To overcome this limitation, we changed the matching strategy. Instead of asking:\n",
    "\n",
    "> \"Which scene feature best matches the model feature?\"\n",
    "\n",
    "we ask:\n",
    "\n",
    "> \"For each scene feature, which model feature does it resemble the most?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ght_sift_lib import SiftGhtDetector, natural_sort_key, format_and_print_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = os.path.join(\"dataset\", \"models\")\n",
    "SCENES_DIR = os.path.join(\"dataset\", \"scenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "detector = SiftGhtDetector(\n",
    "    num_octave_layers=5,\n",
    "    bin_size=6,\n",
    "    ratio_threshold=0.75,\n",
    "    min_votes=3,\n",
    "    nms_window_size=8,\n",
    "    use_clahe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = sorted(\n",
    "    [\n",
    "        f\n",
    "        for f in os.listdir(MODELS_DIR)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ],\n",
    "    key=natural_sort_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate(model_files):\n",
    "    book_name = f\"Book {i}\"\n",
    "    model_path = os.path.join(MODELS_DIR, filename)\n",
    "    models[book_name] = cv2.imread(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_files = sorted(os.listdir(SCENES_DIR), key=natural_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_files = sorted(os.listdir(SCENES_DIR), key=natural_sort_key)\n",
    "\n",
    "for scene_filename in scene_files:\n",
    "    if not scene_filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        continue\n",
    "\n",
    "    scene_path = os.path.join(SCENES_DIR, scene_filename)\n",
    "    scene_image = cv2.imread(scene_path)\n",
    "    overlay_image = scene_image.copy()\n",
    "    all_detections = {}\n",
    "\n",
    "    for book_name, model_image in models.items():\n",
    "        peaks, accumulator, bounding_boxes = detector.detect(model_image, scene_image)\n",
    "\n",
    "        valid_boxes = [b for b in bounding_boxes if b.get(\"bounding_box\") is not None]\n",
    "        if valid_boxes:\n",
    "            all_detections[book_name] = valid_boxes\n",
    "\n",
    "            colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255)]\n",
    "\n",
    "            for i, det in enumerate(valid_boxes):\n",
    "                color = colors[i % len(colors)]\n",
    "                corners = np.int32(det[\"bounding_box\"])\n",
    "                cv2.polylines(\n",
    "                    overlay_image, [corners], isClosed=True, color=color, thickness=2\n",
    "                )\n",
    "\n",
    "    format_and_print_results(all_detections, scene_filename)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(overlay_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(scene_filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, after careful parameter tuning, this system proved capable of detecting 86.8% of the object instances across all scenes (46 out of 53), demonstrating its robustness.\n",
    "\n",
    "One limitation remains, due to […]. Possible improvements could be explored, although this is always challenging when dealing with classical computer vision approaches. While reliable in industrial contexts, such approaches struggle to generalize to scenes that differ significantly from those for which the parameters were tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Zuiderveld, K. (1994). Contrast Limited Adaptive Histogram Equalization. In *Graphics Gems IV* (pp. 474-485), Academic Press Professional.\n",
    "- Lowe, D. G. (2004). Distinctive Image Features from Scale-Invariant Keypoints. *International Journal of Computer Vision*, 60(2), 91-110.\n",
    "- Ballard, D. H. (1981). Generalizing the Hough Transform to detect arbitrary shapes. *Pattern Recognition*, 13(2), 111-122.\n",
    "- Prof. Lisanti's lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ipcv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
